<!-- Generated by SpecStory -->

# Switching to Cloudscraper for HTTP Requests (2025-04-28 21:06:55)

_**User**_

use cloudscraper

---

_**Assistant**_

Let me help you modify the code to use `cloudscraper` for making HTTP requests. I notice that the code currently uses the `requests` library, and we'll need to modify it to use `cloudscraper` to better handle websites with anti-bot protections.

First, I'll examine the code and then make the necessary changes:

```python
# Technical Analysis
- Current implementation uses requests library
- Multiple HTTP requests are made in different functions
- Need to replace requests with cloudscraper
- Need to maintain existing functionality while improving anti-bot bypass
```

Let me modify the code to use `cloudscraper`:

---

```diff
  query = "frontend"
  
+ # Initialize cloudscraper
+ scraper = cloudscraper.create_scraper(
+     browser={
+         'browser': 'chrome',
+         'platform': 'windows',
+         'mobile': False
+     }
+ )
  
  def fetch_data(url):
      """Fetches data from the given URL and returns the response content."""
      try:
-         response = requests.get(url)
+         response = scraper.get(url)
          response.raise_for_status()  # Raise an exception for bad status codes
          return response.text
-     except requests.exceptions.RequestException as e:
+     except Exception as e:
          print(f"Error fetching data from {url}: {e}")
          return None
```

```diff
      while page <= max_pages:
          try:
-             # Construct the API URL with proper parameters
              url = f'https://hk.jobsdb.com/api/jobsearch/v5/search?siteKey=HK-Main&page={page}&pageSize=100&keywords={query}'
-             # print(f"Fetching page {page} for query '{query}'")
              print(url)
  
-             # Make the request
-             response = requests.get(url, headers=get_jobsdb_headers(), proxies=get_random_proxy())
+             # Use cloudscraper instead of requests
+             response = scraper.get(url, headers=get_jobsdb_headers(), proxies=get_random_proxy())
  
-             # Check if the request was successful
              if response.status_code == 200:
-                 # Parse the JSON response
                  response_data = response.json()
- 
-                 # Extract jobs from the data array
                  jobs = response_data.get('data', [])
  
-                 # If no jobs in this page, we've reached the end - move to next query
                  if not jobs:
                      print(f"No more jobs found for '{query}'. Moving to next query.")
                      break
                  print('finded jobs', len(jobs))
-                 # Process each job
+                 
                  for job in jobs:
                      job_id = job.get('id')
```

```diff
                          find_job_detail(job_id)
  
-                 # Move to next page
                  page += 1
  
```

```diff
      for attempt in range(5):
          headers = get_jobsdb_headers_with_session(session_id)
-         response = requests.post(base_url, headers=headers, json=json_data, proxies=get_random_proxy())
+         response = scraper.post(base_url, headers=headers, json=json_data, proxies=get_random_proxy())
  
          try:
              response_json = response.json()
              data = response_json
-             # print(data)
-             # print(get_random_proxy())
              if 'error' in data and 'message' in data['error'] and 'too many requests' in data['error']['message']:
                  print(f'Too many requests. Attempt {attempt + 1} of 5')
```

```diff
              else:
                  break
-             # If we get here, we have a valid response
-             # return data
  
-         except requests.exceptions.JSONDecodeError:
+         except Exception as e:
              print(f"Failed to decode JSON response. Attempt {attempt + 1} of 5")
              if attempt == 4:  # Last attempt
```

```diff
              }
  
-             response = requests.post(url, headers=headers, json=payload, proxies=get_random_proxy())
+             response = scraper.post(url, headers=headers, json=payload, proxies=get_random_proxy())
  
              if response.status_code == 200:
```

---

我已经修改了代码以使用 `cloudscraper` 替代 `requests` 库。主要更改包括：

1. 初始化了一个全局的 `cloudscraper` 实例，配置为模拟 Chrome 浏览器：
```python
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'mobile': False
    }
)
```

2. 替换了所有的 `requests` 调用为 `scraper` 调用：
- `requests.get()` → `scraper.get()`
- `requests.post()` → `scraper.post()`

3. 更新了异常处理，使用更通用的 `Exception` 而不是特定的 `requests` 异常。

这些更改将帮助：
- 绕过反爬虫保护
- 处理 Cloudflare 的挑战
- 提高请求的成功率

代码的其他功能保持不变，包括：
- 代理支持
- 重试机制
- 数据处理逻辑
- 数据库操作

您现在可以运行修改后的代码，它应该能更好地处理网站的反爬虫机制。如果您遇到任何问题，请告诉我，我会帮您进一步调整代码。

---

